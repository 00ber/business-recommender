{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4374f27-4651-40dc-9b69-758249667025",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Found existing installation: protobuf 3.19.3\n",
      "Uninstalling protobuf-3.19.3:\n",
      "  Successfully uninstalled protobuf-3.19.3\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall protobuf -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b58e934b-de66-45ce-9076-c79459d56b51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Requirement already satisfied: torch in /databricks/python3/lib/python3.10/site-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: transformers in /databricks/python3/lib/python3.10/site-packages (4.34.0)\n",
      "Requirement already satisfied: accelerate in /databricks/python3/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: einops in /databricks/python3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: langchain in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (0.0.162)\n",
      "Requirement already satisfied: xformers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (0.0.22)\n",
      "Requirement already satisfied: bitsandbytes in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (0.41.2.post2)\n",
      "Requirement already satisfied: faiss-gpu in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: sentence_transformers in /databricks/python3/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: sentencepiece in /databricks/python3/lib/python3.10/site-packages (0.1.99)\n",
      "Requirement already satisfied: openai==0.27.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (0.27.7)\n",
      "Requirement already satisfied: pinecone-client[grpc]==2.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: tiktoken==0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets==2.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (2.12.0)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /databricks/python3/lib/python3.10/site-packages (from langchain) (1.10.6)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /databricks/python3/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in /databricks/python3/lib/python3.10/site-packages (from langchain) (4.64.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /databricks/python3/lib/python3.10/site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /databricks/python3/lib/python3.10/site-packages (from langchain) (8.1.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in /databricks/python3/lib/python3.10/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /databricks/python3/lib/python3.10/site-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (from langchain) (2.8.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /databricks/python3/lib/python3.10/site-packages (from pinecone-client[grpc]==2.2.1) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from pinecone-client[grpc]==2.2.1) (1.26.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /databricks/python3/lib/python3.10/site-packages (from pinecone-client[grpc]==2.2.1) (4.4.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (from pinecone-client[grpc]==2.2.1) (2.4.2)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (from pinecone-client[grpc]==2.2.1) (0.7.2)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.53.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (from pinecone-client[grpc]==2.2.1) (1.56.4)\n",
      "Requirement already satisfied: lz4>=3.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (from pinecone-client[grpc]==2.2.1) (4.3.2)\n",
      "Collecting protobuf==3.19.3\n",
      "  Using cached protobuf-3.19.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: grpcio>=1.44.0 in /databricks/python3/lib/python3.10/site-packages (from pinecone-client[grpc]==2.2.1) (1.48.2)\n",
      "Requirement already satisfied: grpc-gateway-protoc-gen-openapiv2==0.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages (from pinecone-client[grpc]==2.2.1) (0.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /databricks/python3/lib/python3.10/site-packages (from tiktoken==0.4.0) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /databricks/python3/lib/python3.10/site-packages (from datasets==2.12.0) (0.16.4)\n",
      "Requirement already satisfied: multiprocess in /databricks/python3/lib/python3.10/site-packages (from datasets==2.12.0) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in /databricks/python3/lib/python3.10/site-packages (from datasets==2.12.0) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from datasets==2.12.0) (0.3.6)\n",
      "Requirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from datasets==2.12.0) (22.0)\n",
      "Requirement already satisfied: xxhash in /databricks/python3/lib/python3.10/site-packages (from datasets==2.12.0) (3.4.1)\n",
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (from datasets==2.12.0) (1.5.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /databricks/python3/lib/python3.10/site-packages (from datasets==2.12.0) (8.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /databricks/python3/lib/python3.10/site-packages (from datasets==2.12.0) (2023.6.0)\n",
      "Requirement already satisfied: networkx in /databricks/python3/lib/python3.10/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: sympy in /databricks/python3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /databricks/python3/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: lit in /databricks/python3/lib/python3.10/site-packages (from triton==2.0.0->torch) (17.0.5)\n",
      "Requirement already satisfied: cmake in /databricks/python3/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.27.7)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.14.0)\n",
      "Requirement already satisfied: psutil in /databricks/python3/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from sentence_transformers) (1.10.0)\n",
      "Requirement already satisfied: nltk in /databricks/python3/lib/python3.10/site-packages (from sentence_transformers) (3.7)\n",
      "Requirement already satisfied: scikit-learn in /databricks/python3/lib/python3.10/site-packages (from sentence_transformers) (1.1.1)\n",
      "Requirement already satisfied: torchvision in /databricks/python3/lib/python3.10/site-packages (from sentence_transformers) (0.15.2+cu118)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/lib/python3/dist-packages (from grpcio>=1.44.0->pinecone-client[grpc]==2.2.1) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: click in /databricks/python3/lib/python3.10/site-packages (from nltk->sentence_transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /databricks/python3/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas->datasets==2.12.0) (2022.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /databricks/python3/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "databricks-feature-store 0.16.1 requires pyspark<4,>=3.1.2, which is not installed.\n",
      "tensorflow 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.3 which is incompatible.\n",
      "tensorboard 2.14.0 requires protobuf>=3.19.6, but you have protobuf 3.19.3 which is incompatible.\n",
      "tensorboard-plugin-profile 2.14.0 requires protobuf<5.0.0dev,>=3.19.6, but you have protobuf 3.19.3 which is incompatible.\n",
      "google-api-core 2.14.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
      "facets-overview 1.1.1 requires protobuf>=3.20.0, but you have protobuf 3.19.3 which is incompatible.\n",
      "Successfully installed protobuf-3.19.3\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers sentencepiece openai==0.27.7 \"pinecone-client[grpc]\"==2.2.1 pinecone-client==2.2.1 langchain==0.0.162 tiktoken==0.4.0 datasets==2.12.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a08a01-aa9f-41bb-b9c5-cfe8574cf84c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting protobuf==3.20.3\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.3\n",
      "    Uninstalling protobuf-3.19.3:\n",
      "      Successfully uninstalled protobuf-3.19.3\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "databricks-feature-store 0.16.1 requires pyspark<4,>=3.1.2, which is not installed.\n",
      "Successfully installed protobuf-3.20.3\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install protobuf==3.20.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7fe6748-24ec-446e-936b-4fc42d9aa49a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protobuf==3.20.3\n"
     ]
    }
   ],
   "source": [
    "%pip freeze | grep protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a01bf8f-97a5-437d-90ff-9baf2991a9aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and extract data\n",
    "- Download the Google Local Data (2021) for Maryland along with the metadata for businesses\n",
    "- Save the data to Databricks dbfs to prevent the need for downloading again in subsequent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59d2a65-cd47-4778-a490-cfb0512c4d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "java.lang.IllegalStateException: jupyter client is not available because the python kernel is not defined. The kernel may be restarting or the repl may have been shut down.\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.$anonfun$getJupyterKernelListener$1(JupyterDriverLocal.scala:312)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$getJupyterKernelListener(JupyterDriverLocal.scala:311)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.executePython(JupyterDriverLocal.scala:954)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:842)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$34(DriverLocal.scala:1038)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:1021)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:73)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:73)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:958)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:741)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:733)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:642)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:687)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:519)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:435)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:277)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "java.lang.IllegalStateException: jupyter client is not available because the python kernel is not defined. The kernel may be restarting or the repl may have been shut down.\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.$anonfun$getJupyterKernelListener$1(JupyterDriverLocal.scala:312)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$getJupyterKernelListener(JupyterDriverLocal.scala:311)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.executePython(JupyterDriverLocal.scala:954)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:842)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$34(DriverLocal.scala:1038)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:1021)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:73)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:73)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:958)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:741)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:733)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:642)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:687)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:519)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:435)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:277)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "IllegalStateException: jupyter client is not available because the python kernel is not defined. The kernel may be restarting or the repl may have been shut down.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !wget https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/review-Maryland_10.json.gz -O /tmp/review-Maryland_10.json.gz && gunzip /tmp/review-Maryland_10.json.gz && head -n 1000000 /tmp/review-Maryland_10.json > /tmp/review_1M.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1bb7e56-9d6a-41b6-a2d3-bd2e82a13700",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/meta-Maryland.json.gz -O /tmp/meta-Maryland.json.gz && gunzip /tmp/meta-Maryland.json.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba56a79-c070-454f-bcff-9c06015cf270",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Move files to Databricks file system\n",
    "# dbutils.fs.mv(\"file:/tmp/review_1M.json\", \"dbfs:/FileStore/review-Maryland-1M.json\")\n",
    "# dbutils.fs.mv(\"file:/tmp/review-Maryland_10.json\", \"dbfs:/FileStore/review-Maryland_10.json\")\n",
    "# dbutils.fs.mv(\"file:/tmp/meta-Maryland.json\", \"dbfs:/FileStore/meta-Maryland.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data\n",
    "\n",
    "Create spark dataframes for both reviews data and metadata and join them using gmap_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70056810-fc9e-4227-8c94-6d561197b385",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "review_df = spark.read.json(\"dbfs:/FileStore/review-Maryland-1M.json\")\n",
    "metadata_df = spark.read.json(\"dbfs:/FileStore/meta-Maryland.json\").withColumnsRenamed({'name': 'business_name', 'gmap_id': 'business_gmap_id'})\n",
    "data = review_df.join(\n",
    "    metadata_df, \n",
    "    review_df.gmap_id == metadata_df.business_gmap_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d9b2ac2-81ff-42d0-b3b9-67bd2b09236f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+------+----+--------------------+-------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+----------+-----------+------------------+--------------+-----+--------------------+-----------------+--------------------+\n",
      "|             gmap_id|             name|                pics|rating|resp|                text|         time|             user_id|                MISC|             address|avg_rating|            category|         description|    business_gmap_id|               hours|  latitude|  longitude|     business_name|num_of_reviews|price|    relative_results|            state|                 url|\n",
      "+--------------------+-----------------+--------------------+------+----+--------------------+-------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+----------+-----------+------------------+--------------+-----+--------------------+-----------------+--------------------+\n",
      "|0x4070c4eacf2e403...|           Jane S|                NULL|     5|NULL|This store's grea...|1617747545586|10405663687942029...|{[Wheelchair acce...|Advance Auto Part...|       4.0|[Auto parts store...|Chain stocking a ...|0x4070c4eacf2e403...|[[Monday, 7:30AM–...|39.1691777|-76.7846435|Advance Auto Parts|           238| NULL|[0x89b7e0798b2bb0...|Open ⋅ Closes 9PM|https://www.googl...|\n",
      "|0x4070c4eacf2e403...|Vainuupo Avegalio|[{[https://lh5.go...|     5|NULL|I love this locat...|1475761087908|11781076692153455...|{[Wheelchair acce...|Advance Auto Part...|       4.0|[Auto parts store...|Chain stocking a ...|0x4070c4eacf2e403...|[[Monday, 7:30AM–...|39.1691777|-76.7846435|Advance Auto Parts|           238| NULL|[0x89b7e0798b2bb0...|Open ⋅ Closes 9PM|https://www.googl...|\n",
      "|0x4070c4eacf2e403...|     Ursula Bosic|                NULL|     5|NULL|Needed to replace...|1606677679571|11116871135443834...|{[Wheelchair acce...|Advance Auto Part...|       4.0|[Auto parts store...|Chain stocking a ...|0x4070c4eacf2e403...|[[Monday, 7:30AM–...|39.1691777|-76.7846435|Advance Auto Parts|           238| NULL|[0x89b7e0798b2bb0...|Open ⋅ Closes 9PM|https://www.googl...|\n",
      "|0x4070c4eacf2e403...|  Adeola Williams|[{[https://lh5.go...|     2|NULL|Wouldn't have bou...|1559081880765|11247199726600737...|{[Wheelchair acce...|Advance Auto Part...|       4.0|[Auto parts store...|Chain stocking a ...|0x4070c4eacf2e403...|[[Monday, 7:30AM–...|39.1691777|-76.7846435|Advance Auto Parts|           238| NULL|[0x89b7e0798b2bb0...|Open ⋅ Closes 9PM|https://www.googl...|\n",
      "|0x4070c4eacf2e403...|   jeremy dillard|                NULL|     5|NULL|The staff is real...|1611881466158|11108637177269788...|{[Wheelchair acce...|Advance Auto Part...|       4.0|[Auto parts store...|Chain stocking a ...|0x4070c4eacf2e403...|[[Monday, 7:30AM–...|39.1691777|-76.7846435|Advance Auto Parts|           238| NULL|[0x89b7e0798b2bb0...|Open ⋅ Closes 9PM|https://www.googl...|\n",
      "|0x4070c4eacf2e403...|Gaming with sm014|                NULL|     3|NULL|It's not good but...|1609269904354|10586092793363347...|{[Wheelchair acce...|Advance Auto Part...|       4.0|[Auto parts store...|Chain stocking a ...|0x4070c4eacf2e403...|[[Monday, 7:30AM–...|39.1691777|-76.7846435|Advance Auto Parts|           238| NULL|[0x89b7e0798b2bb0...|Open ⋅ Closes 9PM|https://www.googl...|\n",
      "|0x4070c4eacf2e403...|          It's Me|                NULL|     5|NULL|In/out with what ...|1605225996906|11750071713999091...|{[Wheelchair acce...|Advance Auto Part...|       4.0|[Auto parts store...|Chain stocking a ...|0x4070c4eacf2e403...|[[Monday, 7:30AM–...|39.1691777|-76.7846435|Advance Auto Parts|           238| NULL|[0x89b7e0798b2bb0...|Open ⋅ Closes 9PM|https://www.googl...|\n",
      "|0x4070c4eacf2e403...|    David Sanford|                NULL|     5|NULL|Ordered the part ...|1616538380871|11298738893850995...|{[Wheelchair acce...|Advance Auto Part...|       4.0|[Auto parts store...|Chain stocking a ...|0x4070c4eacf2e403...|[[Monday, 7:30AM–...|39.1691777|-76.7846435|Advance Auto Parts|           238| NULL|[0x89b7e0798b2bb0...|Open ⋅ Closes 9PM|https://www.googl...|\n",
      "|0x4070c4eacf2e403...|    Cris Connelly|                NULL|     5|NULL|Got what i needed...|1599862705930|10075251299754515...|{[Wheelchair acce...|Advance Auto Part...|       4.0|[Auto parts store...|Chain stocking a ...|0x4070c4eacf2e403...|[[Monday, 7:30AM–...|39.1691777|-76.7846435|Advance Auto Parts|           238| NULL|[0x89b7e0798b2bb0...|Open ⋅ Closes 9PM|https://www.googl...|\n",
      "|0x4070c4eacf2e403...|  Daniel Cedillos|                NULL|     5|NULL|One of my favorit...|1599624092839|11679382614311261...|{[Wheelchair acce...|Advance Auto Part...|       4.0|[Auto parts store...|Chain stocking a ...|0x4070c4eacf2e403...|[[Monday, 7:30AM–...|39.1691777|-76.7846435|Advance Auto Parts|           238| NULL|[0x89b7e0798b2bb0...|Open ⋅ Closes 9PM|https://www.googl...|\n",
      "+--------------------+-----------------+--------------------+------+----+--------------------+-------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+----------+-----------+------------------+--------------+-----+--------------------+-----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run pipeline to map data and write to Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a udf to create Documents to use for generating embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b7b42a-0b87-4210-b412-a95dff8130af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "def safe_get(val):\n",
    "  return val if val is not None else \"N/A\"\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def to_id(user_id, gmap_id):\n",
    "    _id  = hashlib.md5(f\"{user_id}|{gmap_id}\".encode(\"utf-8\")).hexdigest()\n",
    "    return _id\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def to_document(address, business_name, review_text, misc, hours, url, latitude, longitude, description, categories, avg_rating):\n",
    "    address = safe_get(address)\n",
    "    business_name = safe_get(business_name)\n",
    "    review_text = safe_get(review_text)[:2000]\n",
    "    misc_str = \"{}\"\n",
    "    if misc is not None:\n",
    "        misc_dict = misc.asDict()\n",
    "        misc_data = {k: v for k, v in misc_dict.items() if v is not None }\n",
    "        misc_str = json.dumps(misc_data)\n",
    "    hours = safe_get(hours)\n",
    "    url = safe_get(url)\n",
    "    lat_long = f\"{latitude}, {longitude}\" if latitude is not None and longitude is not None else \"N/A\"\n",
    "    description = safe_get(description)[:1500]\n",
    "    categories = \", \".join(categories) if categories is not None and len(categories) > 0 else \"N/A\"\n",
    "    rating = f\"{float(avg_rating):.2f}\" if avg_rating else \"N/A\"\n",
    "    doc = f\"### Address: {address}\\n### Name: {business_name}\\n### Review: {review_text}\\n### Average Rating: {rating}\\n### Hours: {hours}\\n### URL: {url}\\n### Description: {description}\\n### Categories: {categories}\\n### Latitude/Longitude: {lat_long}\\n### Hours: {hours}\\n### Misc: {misc_str}\"\n",
    "    return doc\n",
    "\n",
    "pinecone_df = data.select(\n",
    "    to_id(\"user_id\", \"business_gmap_id\").alias(\"document_id\"), \n",
    "    to_document(\"address\", \"business_name\", \"text\", \"MISC\", \"hours\", \"url\", \"latitude\", \"longitude\", \"description\", \"category\", \"avg_rating\" ).alias(\"document\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Pinecone\n",
    "- Initialize index if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a87c034d-7eb0-475c-bfff-1330c888e381",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-2e36b237-37a3-4890-8b3d-a7f6d2eec139/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Setup Pinecone\n",
    "import pinecone\n",
    "\n",
    "PINECONE_API_KEY = dbutils.secrets.get(scope=\"business-retrieval-app\", key=\"PINECONE_API_KEY\")\n",
    "environment = 'us-east-1-aws'\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=environment)\n",
    "\n",
    "index_name = 'business-listings'\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='dotproduct',\n",
    "        dimension=384\n",
    "    )\n",
    "index = pinecone.Index(index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2cb1e3-3287-49b9-a129-0dd26fcd01ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304d80e11b4a44e5b66822d54a584f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808044af7ba748078901c1840fa14d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81aaf2a6263451b86b419fbb025461f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26195e88be52442280b44f62a1fdca1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b5723d0ee2402091929b29eed67e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2608eec6272c41f8b12d8e3237217c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4419417c2954509bf72aecc96a3c80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976fff74e9a843779f08f19a9dc21d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff997c674bc469fa8710d9efc50132e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6fe89eabc140a59566adc4ec63392a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d380aad27246f09a9d9adef4d4ae17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e976c5649128444eb8467119ba56eaaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdb8125b77b404f895ae2af0fda79b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d9deaaf55048b9bf343877cefebbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from typing import Iterator,Tuple\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "\n",
    "embedder = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define map function to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9b03f3-7047-4803-8438-89a9d46e02b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "from typing import TypeAlias\n",
    "import numpy as np\n",
    "\n",
    "def create_embeddings(pdf)-> pd.DataFrame[\"id\": str, \"values\": list[np.float32], \"namespace\": str, \"metadata\": str]:\n",
    "    embedder = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    documents = pdf.document.to_list()\n",
    "    document_ids = pdf.document_id.to_list()\n",
    "    embeddings = embedder.embed_documents(documents)\n",
    "    metadata = [json.dumps({'doc_id': str(row.document_id), 'document': row.document}) for _, row in pdf.iterrows()]\n",
    "    return pd.DataFrame({\"id\": document_ids, \"values\": embeddings, \"namespace\": \"\", \"metadata\": metadata})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447fbfe2-da37-4f71-82f0-c20a85e7488b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mapped_df = pinecone_df.pandas_api().pandas_on_spark.apply_batch(create_embeddings).to_spark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline and write to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed411a4-b1c7-4563-9a00-5b941071329d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "    mapped_df.write\n",
    "    .option(\"pinecone.apiKey\", PINECONE_API_KEY)\n",
    "    .option(\"pinecone.environment\", environment)\n",
    "    .option(\"pinecone.projectName\", pinecone.whoami().projectname)\n",
    "    .option(\"pinecone.indexName\", index_name)\n",
    "    .format(\"io.pinecone.spark.pinecone.Pinecone\")\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "017a3c6b-0d94-460e-89c9-08d49b2e2e5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "generate-embeddings",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
